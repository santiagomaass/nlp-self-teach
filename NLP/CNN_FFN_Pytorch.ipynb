{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwaDuQqCOyLJ"
      },
      "source": [
        "# **Homework 4 - CC6205 Natural Language Processing ðŸ“š**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "In this task, I will implement a Convolutional Neural Network and a Feed Foward Network using PyTorch. This network will address the problem of tagging (generating sequences of labels of the same length as the input sequence).\n",
        "\n",
        "**This code belongs to a Homework from CC6205 - by Gabriel Iturra Bocaz. My code - Santiago Maass - is the Model Section**\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)\n",
        "- [MEMMs and CRFs](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CRF.pdf): [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf): [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf): [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the task, you will need to implement a Chatbot capable of generating a basic conversation using a Star Wars dataset. During the development, it is expected that you can design a bot (which will have a classifier behind it) capable of classifying different labels, so that once the label is identified, it provides a response relevant to the question."
      ],
      "metadata": {
        "id": "GEla92bUymrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_data = pd.read_json('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json')\n",
        "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKOGlMs3Dx-",
        "outputId": "f4fe7046-2323-47ff-9147-08ebaf0428a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tags:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data[\"intents\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-twhnhQjW7i",
        "outputId": "8ef1b21f-fc81-4007-906b-a810f3ae448d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     {'tag': 'greeting', 'patterns': ['Hi', 'Hey', ...\n",
              "1     {'tag': 'goodbye', 'patterns': ['Bye', 'See yo...\n",
              "2     {'tag': 'thanks', 'patterns': ['Thanks', 'Than...\n",
              "3     {'tag': 'tasks', 'patterns': ['What can you do...\n",
              "4     {'tag': 'alive', 'patterns': ['Are you alive.'...\n",
              "5     {'tag': 'Menu', 'patterns': ['Which items do y...\n",
              "6     {'tag': 'help', 'patterns': ['I am looking for...\n",
              "7     {'tag': 'mission', 'patterns': ['I am on missi...\n",
              "8     {'tag': 'jedi', 'patterns': ['Tell me top 10 j...\n",
              "9     {'tag': 'sith', 'patterns': ['Tell me top 10 s...\n",
              "10    {'tag': 'bounti hounter', 'patterns': ['Tell m...\n",
              "11    {'tag': 'funny', 'patterns': ['Tell me a joke!...\n",
              "12    {'tag': 'about me', 'patterns': ['Do you know ...\n",
              "13    {'tag': 'creator', 'patterns': ['Who is your c...\n",
              "14    {'tag': 'myself', 'patterns': ['Tell me about ...\n",
              "15    {'tag': 'stories', 'patterns': ['Tell me a sto...\n",
              "Name: intents, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaciÃ³n, ejemplos del contenido del primer registro:"
      ],
      "metadata": {
        "id": "V-6fCE5fHkNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_data[\"intents\"][0][\"patterns\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axsi27BpHGOx",
        "outputId": "fc781de1-65e4-4a9a-8cc3-1757c4e09f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'Hey',\n",
              " 'How are you',\n",
              " 'Is anyone there?',\n",
              " 'Hello',\n",
              " 'Good day',\n",
              " \"What's up\",\n",
              " 'Yo!',\n",
              " 'Howdy',\n",
              " 'Nice to meet you.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data['intents'][0]['responses']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0vGdwoHeg3",
        "outputId": "b3b54869-a5f4-4731-a15d-e9431a779306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'Hello, thanks for visiting.',\n",
              " 'Hi there, what can I do for you?',\n",
              " 'Hi there, how can I help?',\n",
              " 'Hello, there.',\n",
              " 'Hello Dear',\n",
              " 'Ooooo Hello, looking for someone or something?',\n",
              " 'Yes, I am here.',\n",
              " 'Listening carefully.',\n",
              " 'Ok, I am with you.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data['intents'][0]['tag']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0BnYez1oGtx3",
        "outputId": "a153a65a-7bc5-42f1-cd8c-be023cf1c619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'greeting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the loaded dataset, we can notice that it comes in a JSON format, meaning that its data is stored in dictionaries. The keys of the dictionaries are not random; they serve to identify relevant points in the bot's development. Here's a brief description of the keys:\n",
        "\n",
        "patterns: It stores the patterns used to train the model ðŸ˜®. In other words, it is the training corpus that contains only questions or expressions that the bot should respond to.\n",
        "responses: These are the corresponding responses ðŸ™‹ to the patterns. We will use them in a later stage after classification to provide a random response to the user.\n",
        "tag: These are the labels used to train our model ðŸ’».\n",
        "In summary, the relevant keys for training our neural network will be patterns (corpus) and tag (labels)."
      ],
      "metadata": {
        "id": "v6BvAWCw3zPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install and import"
      ],
      "metadata": {
        "id": "RUwxivx2MpMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto toma su tiempo en ejecutarse\n",
        "%%capture\n",
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchtext==0.9.0"
      ],
      "metadata": {
        "id": "TjSZkBsk1H4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from random import choice\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "RfZ6SL-Q1Kwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset ðŸ“š"
      ],
      "metadata": {
        "id": "oj-Epe7XJLrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we obtain the dataset\n",
        "!wget 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvlLqYRrVN6l",
        "outputId": "efabba73-2b32-4f39-ebcc-434fcf57bf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-15 19:23:10--  https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14469 (14K) [text/plain]\n",
            "Saving to: â€˜star_wars_chatbot.jsonâ€™\n",
            "\n",
            "\rstar_wars_chatbot.j   0%[                    ]       0  --.-KB/s               \rstar_wars_chatbot.j 100%[===================>]  14.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-15 19:23:10 (75.0 MB/s) - â€˜star_wars_chatbot.jsonâ€™ saved [14469/14469]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using json\n",
        "with open('star_wars_chatbot.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Create a vocab with the dataset and get the number of classes that have\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
        "num_classes = len(dataset['intents'])\n",
        "vocab.set_default_index(0)\n",
        "vocab.insert_token('<pad>', 1)\n",
        "# Define a list with the labels\n",
        "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
        "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
        "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
      ],
      "metadata": {
        "id": "MbbIsFUG1TXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model"
      ],
      "metadata": {
        "id": "a52SUNKPJQxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Added function to fix padding in foward step\n",
        "def zip_longest_(text, offsets, vocab, window_len):\n",
        "    \"\"\"\n",
        "    Zip longest function for iterating over sliding windows of text.\n",
        "    Combinated with zip(), its used to add padding for the sentences\n",
        "    shorter than the window length.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        offsets (List[int]): Offsets indicating the start positions of each window.\n",
        "        vocab (Dict[str, Any]): Vocabulary dictionary.\n",
        "        window_len (int): Length of the sliding window.\n",
        "\n",
        "    Yields:\n",
        "        Tuple: A tuple containing the elements of the sliding window.\n",
        "\n",
        "    \"\"\"\n",
        "    items = window_len\n",
        "    iterables = ([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(text)]])\n",
        "    for iterable in iterables:\n",
        "        items = max(items, len(iterable))\n",
        "\n",
        "    iters = [iter(iterable) for iterable in iterables]\n",
        "    while items:\n",
        "        yield (*[next(i, vocab[\"<pad>\"]) for i in iters],)\n",
        "        items -= 1\n",
        "\n"
      ],
      "metadata": {
        "id": "Is9mKC95e7xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {\"Hoy\": 1, \"es\": 2, \"un\":3, \"lindo\":4, \"dia\":5, \"el\":6, \"sol\":7, \"esta\":8, \"brillando\":9, \"<pad>\":0}\n",
        "v = build_vocab_from_iterator(tokenizer(x) for x in d.keys())\n",
        "print(\"Return of *zip_longest_ for windows of size 2:  \")\n",
        "print(*zip_longest_([\"Hoy\", \"es\", \"un\" ,\"lindo\" ,\"dia\", \"el\" ,\"sol\" ,\"esta\" ,\"brillando\"], [0, 2, 4, 6, 8], v, 2))\n",
        "print(\"Return of zip previous result:  \")\n",
        "print(list(zip(*zip_longest_([\"Hoy\", \"es\", \"un\" ,\"lindo\" ,\"dia\", \"el\" ,\"sol\" ,\"esta\" ,\"brillando\"], [0, 2, 4, 6, 8], v, 2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWgsXFVYpI30",
        "outputId": "dcb3170c-e9fe-46e1-a48e-3e82ab2dbf03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return of *zip_longest_ for windows of size 2:  \n",
            "('Hoy', 'un', 'dia', 'sol', 'brillando') ('es', 'lindo', 'el', 'esta', 0)\n",
            "Return of zip previous result:  \n",
            "[('Hoy', 'es'), ('un', 'lindo'), ('dia', 'el'), ('sol', 'esta'), ('brillando', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10,\n",
        "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "\n",
        "      \"\"\"\n",
        "        CNNClassifier is a PyTorch model that can be either a Convolutional Neural Network (CNN)\n",
        "        or a Feed Forward Neural Network (FFN) for text classification.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            embed_dim (int, optional): Dimensionality of the word embeddings. Defaults to 32.\n",
        "            num_classes (int, optional): Number of output classes. Defaults to 10.\n",
        "            use_cnn (bool, optional): Whether to use the CNN architecture. If False, FFN is used. Defaults to False.\n",
        "            cnn_pool_channels (int, optional): Number of output channels in the CNN pooling layer. Only used if `use_cnn` is True. Defaults to 24.\n",
        "            cnn_kernel_size (int, optional): Size of the CNN kernel. Only used if `use_cnn` is True. Defaults to 3.\n",
        "        \"\"\"\n",
        "      super().__init__()\n",
        "      self.use_cnn = use_cnn\n",
        "      self.window_len = cnn_kernel_size\n",
        "\n",
        "      pad_idx = 1\n",
        "\n",
        "      if self.use_cnn:\n",
        "          # Model is a CNN\n",
        "          self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "          self.conv = nn.Conv1d(\n",
        "              in_channels=1,\n",
        "              out_channels=cnn_pool_channels,\n",
        "              kernel_size=cnn_kernel_size * embed_dim,\n",
        "              stride=embed_dim,\n",
        "          )\n",
        "          fc_in_size = cnn_pool_channels\n",
        "      else:\n",
        "          # Model is a FFN\n",
        "          self.embedding = nn.Embedding(vocab_size, embed_dim, pad_idx)\n",
        "          fc_in_size = embed_dim\n",
        "\n",
        "      self.fc = nn.Linear(fc_in_size, num_classes)\n",
        "      self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes the weights of the model's parameters.\n",
        "        \"\"\"\n",
        "        initrange = 0.5\n",
        "\n",
        "        # Initialize embedding weights\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Initialize linear layer weights\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        if self.use_cnn:\n",
        "            # Initialize convolutional layer weights\n",
        "            self.conv.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Initialize linear layer biases\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "      \"\"\"\n",
        "        Performs forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            text (torch.Tensor): Input text data of shape (batch_size, seq_len).\n",
        "            offsets (List[int]): List of offsets indicating the start positions of each sequence in the batch.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log-probabilities of the predicted classes.\n",
        "      \"\"\"\n",
        "\n",
        "      if self.use_cnn:\n",
        "          # CNN forward pass\n",
        "          text = torch.tensor(\n",
        "                list(\n",
        "                    zip(\n",
        "                        *zip_longest_(text, offsets, vocab, self.window_len)\n",
        "                    )\n",
        "                )\n",
        "            ).to(text.device)\n",
        "          h = self.embedding(text)\n",
        "          h = h.view(h.size(0), 1, -1)\n",
        "          h = torch.relu(self.conv(h))\n",
        "          h = h.mean(dim=2)\n",
        "          output = self.fc(h)\n",
        "          return F.log_softmax(output, dim=1)\n",
        "\n",
        "      else:\n",
        "          # FFN forward pass\n",
        "\n",
        "          # (B, N, 1) -> (B, N, E)\n",
        "          text = torch.tensor(\n",
        "                list(\n",
        "                    zip(\n",
        "                        *zip_longest(\n",
        "                            *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]),\n",
        "                            fillvalue=torch.tensor(vocab[\"<pad>\"])\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            ).to(text.device)\n",
        "\n",
        "          h = self.embedding(text)\n",
        "\n",
        "          # Document representation will be mean of embeddings\n",
        "          h = h.mean(dim=1)\n",
        "\n",
        "          output = self.fc(h)\n",
        "          return F.log_softmax(output, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "n-vQ24tMJG5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### FunciÃ³n Batch"
      ],
      "metadata": {
        "id": "dGN-T0JoJtmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina su funciÃ³n de BATCH\n",
        "stoi = vocab.get_stoi() # Mapea tokens a indices\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0] for entry in batch])\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  # offsets indica en que posiciÃ³n inicia cada oraciÃ³n, donde cada posiciÃ³n es una palabra\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "  # big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
        "\n",
        "  return big_text, offsets, label"
      ],
      "metadata": {
        "id": "K1AZpXc7JxTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 1\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = True\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: Â¿What is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "  # print(len(t) for t in texts)\n",
        "  # print(texts, offsets, cls)\n",
        "  output = model(texts, offsets)\n",
        "  # print(output.shape)\n",
        "  # break"
      ],
      "metadata": {
        "id": "5cXW0XyCrT0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training ðŸ¥Š"
      ],
      "metadata": {
        "id": "YChwpNrrNRBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = False\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: Â¿What is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5eRWRD_J0Km",
        "outputId": "c2234d56-2fa5-494d-99be-dfc6514a162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is avaible: cpu\n",
            "train: 97 elements\n",
            "Epoch: 1000 \t iter-Loss: 0.001final loss: 0.0009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Let's Test! ðŸ§ª"
      ],
      "metadata": {
        "id": "9dlS4_X-L3DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
        "\n",
        "X = torch.tensor([vocab.get_stoi()[t] for t in tokenizer(qText)]).to(device)\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6IhhAKFXL3eH",
        "outputId": "fde79e7f-abad-4d37-ce39-63f7cb21d176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save model ðŸ¦º (optional)"
      ],
      "metadata": {
        "id": "OpSYGx2tL0tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We save de model using pytorch (this is optional, just to learn how to do this in pytorch)\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": INPUT_SIZE,\n",
        "\"output_size\": OUTPUT_SIZE,\n",
        "\"use_cnn\": USE_CNN,\n",
        "\"labels\": labels\n",
        "        }\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "id": "ZBC4TyiqLzDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa139797-f269-4b38-c3ce-452dc2bb7d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chatbot ðŸ’¬"
      ],
      "metadata": {
        "id": "ZYClbTtsMCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('star_wars_chatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    q_text = q_text\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    X = torch.tensor([vocab.get_stoi()[t] if t in stoi else 0 for t in tokenizer(q_text)]).to(device) # se modificÃ³ esta linea para que el chatbot no se caiga al evaluar palabras fuera del vocabulario\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ],
      "metadata": {
        "id": "c249zUwiMBxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b55176-c046-4847-a42e-0746f1bcdb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'finish_chat' to finish the chat)\n",
            "You: hey there\n",
            "GA-97: Hello Dear\n",
            "You: hey hey\n",
            "GA-97: Hello Dear\n",
            "You: any jokes?\n",
            "GA-97: It so dangerous, the most brave sith in galaxy Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.\n",
            "You: tell me something very funny\n",
            "GA-97: You would get bored if I do so.\n",
            "You: what do you have to drink?\n",
            "GA-97: No coffe and no tea, only: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.\n",
            "You: offer me your menu\n",
            "GA-97: My model can't understand you...\n",
            "You: whats on your menu\n",
            "GA-97: My model can't understand you...\n",
            "You: which is the best jedi\n",
            "GA-97: Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto.\n",
            "You: ok bye\n",
            "GA-97: May the force be with you!\n",
            "You: byebye\n",
            "GA-97: See you later, thanks for visiting.\n",
            "You: see you later, alligator\n",
            "GA-97: So, till next time.\n",
            "You: finish_chat\n"
          ]
        }
      ]
    }
  ]
}